{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2431bb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import numpy as np\n",
    " \n",
    "env=gym.make(\"FrozenLake-v1\", render_mode=\"human\", is_slippery=False)\n",
    "env.reset()\n",
    "# render the environment\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "022b4abc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(16)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# observation space - states \n",
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ab772db",
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = {0: \"left\",\n",
    "          1: \"down\",\n",
    "          2: \"right\",\n",
    "          3: \"up\"}\n",
    "\n",
    "def is_square(x):\n",
    "    square_root = math.sqrt(x)\n",
    "    return False if square_root - (square_root // 1) > 0 else True\n",
    "\n",
    "def visualize_actions(prev_a, next_a, action):\n",
    "    if not is_square(env.observation_space.n):\n",
    "        return Exception(\"Observation space is not a square grid\")\n",
    "    else:\n",
    "        n = int(math.sqrt(env.observation_space.n))\n",
    "    \n",
    "    next_obs = next_a\n",
    "    prev_obs = prev_a if prev_a else None\n",
    "    \n",
    "    grid = []\n",
    "    for i in range(n):\n",
    "        inner_grid = []\n",
    "        for j in range(n):\n",
    "            curr_tile = (i * 4 + j)\n",
    "            to_append = \"\"\n",
    "            if next_obs == curr_tile:\n",
    "                to_append = \" X \"\n",
    "            if prev_obs == curr_tile:\n",
    "                if action == 0:\n",
    "                    to_append = \" ← \"\n",
    "                if action == 1:\n",
    "                    to_append = \" ↓ \"\n",
    "                if action == 2:\n",
    "                    to_append = \" → \"\n",
    "                if action == 3:\n",
    "                    to_append = \" ↑ \"\n",
    "            if next_obs != curr_tile and prev_obs != curr_tile:\n",
    "                to_append = \" O \"\n",
    "            inner_grid.append(to_append)\n",
    "                    \n",
    "        grid.append(inner_grid)\n",
    "        print(inner_grid)\n",
    "    \n",
    "    return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69431aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(4)\n"
     ]
    }
   ],
   "source": [
    "# actions: left: 0, down: 1, right: 2, up: 3\n",
    "# Discrete(N) generates a sequence of integers from 0 to N-1 with a step of 1\n",
    "# Therefore, Discrete(4) is equivalant to {0, 1, 2, 3}\n",
    "print(env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f41c2f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------STEP 0-----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Lordf/anaconda3/lib/python3.11/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' O ', ' O ', ' O ', ' O ']\n",
      "[' O ', ' O ', ' O ', ' O ']\n",
      "[' O ', ' O ', ' O ', ' O ']\n",
      "[' O ', ' O ', ' O ', ' O ']\n",
      "----------STEP 1-----------\n",
      "[' O ', ' O ', ' O ', ' O ']\n",
      "[' O ', ' O ', ' O ', ' O ']\n",
      "[' O ', ' O ', ' O ', ' O ']\n",
      "[' O ', ' O ', ' O ', ' O ']\n",
      "DEATH\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5, 0.0, True, False, {'prob': 1.0})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()\n",
    "# .sample() can be used on both Discrete and Space objects to choose a sample randomly\n",
    "# accepts \"mask\" as a parameter which blocks out some options for sampling eg. [0, 1, 1] blocks the 1st option\n",
    "\n",
    "# format of returnValue is (observation,reward, terminated, truncated, info)\n",
    "# observation (object)  - observed state\n",
    "# reward (float)        - reward that is the result of taking the action\n",
    "# terminated (bool)     - is it a terminal state\n",
    "# truncated (bool)      - it is not important in our case\n",
    "# info (dictionary)     - in our case transition probability\n",
    "\n",
    "prev_action = None\n",
    "for i in range(50):\n",
    "    print(f\"----------STEP {i}-----------\")\n",
    "    if i > 0:\n",
    "        prev_action = return_value\n",
    "    random_action= env.action_space.sample()\n",
    "    return_value = env.step(random_action)\n",
    "    visualize_actions(prev_action, return_value, random_action)\n",
    "    if return_value[2]:\n",
    "\n",
    "        if return_value[1] == 0:\n",
    "            print(\"DEATH\")\n",
    "        else:\n",
    "            print(\"VICTORY\")\n",
    "        break\n",
    " \n",
    "env.render()\n",
    "return_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2bc05cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6a58722",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QTableAgent:\n",
    "\n",
    "    def __init__(self, n_states, n_actions, epsilon, learning_rate, discount_value):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_value = discount_value\n",
    "        self.n_states = n_states\n",
    "        self.n_actions = n_actions\n",
    "        self.epsilon = epsilon\n",
    "        self.initialize_q_table(n_states, n_actions)\n",
    "    \n",
    "    \n",
    "    # This is the policy\n",
    "    def choose_best_action(self, state):\n",
    "        '''parameters: state \n",
    "        returns: action'''\n",
    "        # Always need choose_action\n",
    "        return np.argmax(self.q_table[state]) # <-- returns the index with the highest action value for that state\n",
    "    \n",
    "    def choose_action(self, state): # Policy\n",
    "        if np.random.rand(1) < self.epsilon:\n",
    "            action = np.random.randint(0, self.n_actions)\n",
    "        else:\n",
    "            action = self.choose_best_action(state)\n",
    "            \n",
    "        return action\n",
    "    \n",
    "        \n",
    "    def initialize_q_table(self, n_states, n_actions):\n",
    "        self.q_table = np.zeros((n_states, n_actions))\n",
    "        \n",
    "       \n",
    "    \n",
    "    # HOMEWORK\n",
    "    def bellman_equation(self, state, action, next_state, reward):\n",
    "        current_q = self.q_table[state][action]\n",
    "        max_q_next_state = np.max(self.q_table[next_state])\n",
    "        new_q =  current_q + self.learning_rate * (reward + (self.discount_value * max_q_next_state) - current_q)\n",
    "        # single step forecasting?\n",
    "        # value of state = current value + immediate reward + negative or positive expected reward for that state\n",
    "        # reward - immediate reward for moving to next_state? 0 for everything except the goal node (1)\n",
    "        # max_q_next_state - max value of state for all of its actions?\n",
    "        return new_q\n",
    "        \n",
    "    \n",
    "    def update_q_table(self, state, action, next_state, reward):\n",
    "        new_q = self.bellman_equation(state, action, next_state, reward)\n",
    "        self.q_table[state][action] = new_q\n",
    "        \n",
    "    \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2c2b6839",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {0: [(1.0, 0, 0.0, False)],\n",
       "  1: [(1.0, 4, 0.0, False)],\n",
       "  2: [(1.0, 1, 0.0, False)],\n",
       "  3: [(1.0, 0, 0.0, False)]},\n",
       " 1: {0: [(1.0, 0, 0.0, False)],\n",
       "  1: [(1.0, 5, 0.0, True)],\n",
       "  2: [(1.0, 2, 0.0, False)],\n",
       "  3: [(1.0, 1, 0.0, False)]},\n",
       " 2: {0: [(1.0, 1, 0.0, False)],\n",
       "  1: [(1.0, 6, 0.0, False)],\n",
       "  2: [(1.0, 3, 0.0, False)],\n",
       "  3: [(1.0, 2, 0.0, False)]},\n",
       " 3: {0: [(1.0, 2, 0.0, False)],\n",
       "  1: [(1.0, 7, 0.0, True)],\n",
       "  2: [(1.0, 3, 0.0, False)],\n",
       "  3: [(1.0, 3, 0.0, False)]},\n",
       " 4: {0: [(1.0, 4, 0.0, False)],\n",
       "  1: [(1.0, 8, 0.0, False)],\n",
       "  2: [(1.0, 5, 0.0, True)],\n",
       "  3: [(1.0, 0, 0.0, False)]},\n",
       " 5: {0: [(1.0, 5, 0, True)],\n",
       "  1: [(1.0, 5, 0, True)],\n",
       "  2: [(1.0, 5, 0, True)],\n",
       "  3: [(1.0, 5, 0, True)]},\n",
       " 6: {0: [(1.0, 5, 0.0, True)],\n",
       "  1: [(1.0, 10, 0.0, False)],\n",
       "  2: [(1.0, 7, 0.0, True)],\n",
       "  3: [(1.0, 2, 0.0, False)]},\n",
       " 7: {0: [(1.0, 7, 0, True)],\n",
       "  1: [(1.0, 7, 0, True)],\n",
       "  2: [(1.0, 7, 0, True)],\n",
       "  3: [(1.0, 7, 0, True)]},\n",
       " 8: {0: [(1.0, 8, 0.0, False)],\n",
       "  1: [(1.0, 12, 0.0, True)],\n",
       "  2: [(1.0, 9, 0.0, False)],\n",
       "  3: [(1.0, 4, 0.0, False)]},\n",
       " 9: {0: [(1.0, 8, 0.0, False)],\n",
       "  1: [(1.0, 13, 0.0, False)],\n",
       "  2: [(1.0, 10, 0.0, False)],\n",
       "  3: [(1.0, 5, 0.0, True)]},\n",
       " 10: {0: [(1.0, 9, 0.0, False)],\n",
       "  1: [(1.0, 14, 0.0, False)],\n",
       "  2: [(1.0, 11, 0.0, True)],\n",
       "  3: [(1.0, 6, 0.0, False)]},\n",
       " 11: {0: [(1.0, 11, 0, True)],\n",
       "  1: [(1.0, 11, 0, True)],\n",
       "  2: [(1.0, 11, 0, True)],\n",
       "  3: [(1.0, 11, 0, True)]},\n",
       " 12: {0: [(1.0, 12, 0, True)],\n",
       "  1: [(1.0, 12, 0, True)],\n",
       "  2: [(1.0, 12, 0, True)],\n",
       "  3: [(1.0, 12, 0, True)]},\n",
       " 13: {0: [(1.0, 12, 0.0, True)],\n",
       "  1: [(1.0, 13, 0.0, False)],\n",
       "  2: [(1.0, 14, 0.0, False)],\n",
       "  3: [(1.0, 9, 0.0, False)]},\n",
       " 14: {0: [(1.0, 13, 0.0, False)],\n",
       "  1: [(1.0, 14, 0.0, False)],\n",
       "  2: [(1.0, 15, 1.0, True)],\n",
       "  3: [(1.0, 10, 0.0, False)]},\n",
       " 15: {0: [(1.0, 15, 0, True)],\n",
       "  1: [(1.0, 15, 0, True)],\n",
       "  2: [(1.0, 15, 0, True)],\n",
       "  3: [(1.0, 15, 0, True)]}}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77614e7d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'QTableAgent' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 14\u001b[0m\n\u001b[1;32m     10\u001b[0m discount_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.95\u001b[39m\n\u001b[1;32m     12\u001b[0m render_decay \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10000\u001b[39m\n\u001b[0;32m---> 14\u001b[0m agent \u001b[38;5;241m=\u001b[39m QTableAgent(env\u001b[38;5;241m.\u001b[39mobservation_space\u001b[38;5;241m.\u001b[39mn, env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mn, start_epsilon, learning_rate, discount_rate)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m episode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(episodes):\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (episode \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m render_decay \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'QTableAgent' is not defined"
     ]
    }
   ],
   "source": [
    "episodes = 100000\n",
    "\n",
    "start_epsilon = 0.5\n",
    "min_epsilon = 0.1\n",
    "epsilon_reduction = 0.1\n",
    "epsilon_decay = 1000\n",
    "\n",
    "\n",
    "learning_rate = 0.1\n",
    "discount_rate = 0.95\n",
    "\n",
    "render_decay = 10000\n",
    "\n",
    "agent = QTableAgent(env.observation_space.n, env.action_space.n, start_epsilon, learning_rate, discount_rate)\n",
    "\n",
    "for episode in range(episodes):\n",
    "    \n",
    "    if (episode + 1) % render_decay == 0:\n",
    "        env=gym.make(\"FrozenLake-v1\", render_mode=\"human\", is_slippery=False)\n",
    "        visualize_actions\n",
    "    else:\n",
    "        env=gym.make(\"FrozenLake-v1\", render_mode=\"rgb_array\", is_slippery=False)\n",
    "        \n",
    "    if (episode + 1) % epsilon_decay == 0 and (agent.epsilon - epsilon_reduction) > min_epsilon:\n",
    "        agent.epsilon -= epsilon_reduction\n",
    "    \n",
    "    state, _ = env.reset()\n",
    "    terminated, truncated = False, False\n",
    "    \n",
    "    while not terminated and not truncated:\n",
    "        \n",
    "      \n",
    "        action = agent.choose_action(state)\n",
    "    \n",
    "        new_state, reward, terminated, truncated, info = env.step(action)\n",
    "        agent.update_q_table(state, action, new_state, reward)\n",
    "        \n",
    "        if episode == episodes - 1:\n",
    "            visualize_actions(state, new_state, action)\n",
    "            print(\"----------------\")\n",
    "            \n",
    "        state = new_state\n",
    "            \n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "34eaa4cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.73509189, 0.77378094, 0.77378094, 0.73509189],\n",
       "       [0.73509189, 0.        , 0.81450625, 0.77378094],\n",
       "       [0.77378094, 0.857375  , 0.77378094, 0.81450625],\n",
       "       [0.81450625, 0.        , 0.73333693, 0.73457744],\n",
       "       [0.77378094, 0.81450625, 0.        , 0.73509189],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.9025    , 0.        , 0.81450625],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.81450625, 0.        , 0.857375  , 0.77378094],\n",
       "       [0.81450625, 0.9025    , 0.9025    , 0.        ],\n",
       "       [0.857375  , 0.95      , 0.        , 0.857375  ],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.9025    , 0.95      , 0.857375  ],\n",
       "       [0.9025    , 0.95      , 1.        , 0.9025    ],\n",
       "       [0.        , 0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2281b55d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 0.0, False, False, {'prob': 1.0})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3563dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For large state-action spaces, deep learning must be used to estimate the value function"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
