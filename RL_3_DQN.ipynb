{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "843f933f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from random import sample\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc4828e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "\n",
    "    def __init__(self, n_states, n_actions, epsilon, learning_rate, discount_value):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_value = discount_value\n",
    "        self.n_states = n_states\n",
    "        self.n_actions = n_actions\n",
    "        self.epsilon = epsilon\n",
    "        self.initialize_q_table(n_states, n_actions)\n",
    "    \n",
    "    \n",
    "    # This is the policy\n",
    "    def choose_best_action(self, state):\n",
    "        '''parameters: state \n",
    "        returns: action'''\n",
    "        # Always need choose_action\n",
    "        return np.argmax(self.q_table[state]) # <-- returns the index with the highest action value for that state\n",
    "    \n",
    "    def choose_action(self, state): # Policy\n",
    "        if np.random.rand(1) < self.epsilon:\n",
    "            action = np.random.randint(0, self.n_actions)\n",
    "        else:\n",
    "            action = self.choose_best_action(state)\n",
    "            \n",
    "        return action\n",
    "    \n",
    "        \n",
    "    def initialize_q_table(self, n_states, n_actions):\n",
    "        self.q_table = np.zeros((n_states, n_actions))\n",
    "        \n",
    "       \n",
    "    \n",
    "    # HOMEWORK\n",
    "    def bellman_equation(self, state, action, next_state, reward):\n",
    "        current_q = self.q_table[state][action]\n",
    "        max_q_next_state = np.max(self.q_table[next_state])\n",
    "        new_q =  current_q + self.learning_rate * (reward + (self.discount_value * max_q_next_state) - current_q)\n",
    "        # single step forecasting?\n",
    "        # value of state = current value + immediate reward + negative or positive expected reward for that state\n",
    "        # reward - immediate reward for moving to next_state? 0 for everything except the goal node (1)\n",
    "        # max_q_next_state - max value of state for all of its actions?\n",
    "        return new_q\n",
    "        \n",
    "    \n",
    "    def update_q_table(self, state, action, next_state, reward):\n",
    "        new_q = self.bellman_equation(state, action, next_state, reward)\n",
    "        self.q_table[state][action] = new_q\n",
    "        \n",
    "    \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7f3a6a3f",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Perhaps you forgot a comma? (2529935670.py, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[15], line 8\u001b[0;36m\u001b[0m\n\u001b[0;31m    nn.ReLU()\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax. Perhaps you forgot a comma?\n"
     ]
    }
   ],
   "source": [
    "class FCNeuralNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, x_size, y_size, hidden_size):\n",
    "        super.__init__()\n",
    "        \n",
    "        self.linear_relu_nn = nn.Sequential(\n",
    "            nn.Linear(x_size, hidden_size),\n",
    "            nn.ReLU()\n",
    "            nn.Linear(hidden_size, hidden_size)\n",
    "            nn.ReLU()\n",
    "            nn.Linear(hidden_size, y_size)\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        logits = self.linear_relu_nn(x)\n",
    "        return out\n",
    "        \n",
    "    \n",
    "    \n",
    "class ReplayBuffer:\n",
    "    \n",
    "    def __init__(self, limit):\n",
    "        self.buffer = deque([], maxlen = limit)\n",
    "        \n",
    "    def append(self, transition):\n",
    "        self.buffer.append(transition)\n",
    "        \n",
    "    def sample(self, sample_size):\n",
    "        return sample(self.buffer, sample_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "    \n",
    "    \n",
    " \n",
    "    \n",
    "\n",
    "class DQNAgent(Agent): \n",
    "    \n",
    "    # Hyperparameters\n",
    "    self.discount_factor = 0.95\n",
    "    \n",
    "    def __init__(self,  num_actions, num_states, model, replay_buffer)\n",
    "    \n",
    "    self.num_states = num_states,\n",
    "    self.num_actions = num_actions\n",
    "    self.model = model\n",
    "    self.replay_buffer = replay_buffer\n",
    "    \n",
    "    \n",
    "    # Target model initialisation\n",
    "    self.target_model = model\n",
    "    self.target_model.set_weights = self.model.get_weights()\n",
    "    \n",
    "    def choose_best_action(self, state):\n",
    "        # pass through network\n",
    "        action = max(self.model(state))\n",
    "        return action\n",
    "    \n",
    "    def choose_action(self):\n",
    "        if np.random.rand(1) < self.epsilon:\n",
    "            action = np.random.randint(0, self.n_actions)\n",
    "        else:\n",
    "            action = max(self.model(state))\n",
    "        return action\n",
    "    \n",
    "    def learn(self, state, action, next_state, reward):\n",
    "        # Q value predicted for that state, action pair\n",
    "        predicted_value = self.model(state)[action] # predicted value = Q(s, a)\n",
    "        \n",
    "        target_value = reward + self.discount_factor * max(self.model(next_state))  # target value = r + Î³Q(s', a')\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "## TO DO\n",
    "# FIGURE OUT LOSS FUNCTION AND BACKPROPOGATION\n",
    "# IMPLEMENT OPTIONAL BUFFER SAMPLING\n",
    "# IMPLEMENT OPTIONAL TARGET NETWORK\n",
    "# USE MATPLOTLIB OR SOMETHING TO VISUALIZE THE PERFORMANCE OF THE THREE AGENT TYPES\n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1fb917",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0536e718",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (712806370.py, line 37)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[11], line 37\u001b[0;36m\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "episodes = 3000\n",
    "\n",
    "# Epsilon stuff\n",
    "start_epsilon = 0.3\n",
    "min_epsilon = 0.1\n",
    "epsilon_reduction = 0.05\n",
    "epsilon_decay = 100\n",
    "\n",
    "# Visualization\n",
    "render_decay = 1000\n",
    "\n",
    "# Environment setup\n",
    "env_name = \"FrozenLake-v1\"\n",
    "env = gym.make(env_name, render_mode=\"human\")\n",
    "num_actions = env.action_space.N\n",
    "num_states = env.observation_space.N\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "\n",
    "\n",
    "# Q-Table hyperparameters\n",
    "learning_rate = 0.1\n",
    "discount_rate = 0.95\n",
    "\n",
    "\n",
    "# Neural Network hyperparameters\n",
    "hidden_size = 128\n",
    "\n",
    "# Neural network setup\n",
    "model = FCNeuralNetwork(num_actions, num_states, hidden_size)\n",
    "\n",
    "# Replay buffer setup\n",
    "replay_buffer = ReplayBuffer()\n",
    "\n",
    "# Agent setup\n",
    "agent = DQN_Agent(num_actions, num_states, model)\n",
    "\n",
    "for episode in range(episodes):\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5428be7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194d3713",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
