{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "843f933f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import numpy as np\n",
    "from numpy import argmax\n",
    "import torch\n",
    "from torch import nn\n",
    "from random import sample\n",
    "from collections import deque\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bc4828e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "\n",
    "    def __init__(self, n_states, n_actions, epsilon, learning_rate, discount_value):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_value = discount_value\n",
    "        self.n_states = n_states\n",
    "        self.n_actions = n_actions\n",
    "        self.epsilon = epsilon\n",
    "        self.initialize_q_table(n_states, n_actions)\n",
    "    \n",
    "    \n",
    "    # This is the policy\n",
    "    def choose_best_action(self, state):\n",
    "        '''parameters: state \n",
    "        returns: action'''\n",
    "        # Always need choose_action\n",
    "        return np.argmax(self.q_table[state]) # <-- returns the index with the highest action value for that state\n",
    "    \n",
    "    def choose_action(self, state): # Policy\n",
    "        if np.random.rand(1) < self.epsilon:\n",
    "            action = np.random.randint(0, self.n_actions)\n",
    "        else:\n",
    "            action = self.choose_best_action(state)\n",
    "            \n",
    "        return action\n",
    "    \n",
    "        \n",
    "    def initialize_q_table(self, n_states, n_actions):\n",
    "        self.q_table = np.zeros((n_states, n_actions))\n",
    "        \n",
    "       \n",
    "    \n",
    "    # HOMEWORK\n",
    "    def bellman_equation(self, state, action, next_state, reward):\n",
    "        current_q = self.q_table[state][action]\n",
    "        max_q_next_state = np.max(self.q_table[next_state])\n",
    "        new_q =  current_q + self.learning_rate * (reward + (self.discount_value * max_q_next_state) - current_q)\n",
    "        # single step forecasting?\n",
    "        # value of state = current value + immediate reward + negative or positive expected reward for that state\n",
    "        # reward - immediate reward for moving to next_state? 0 for everything except the goal node (1)\n",
    "        # max_q_next_state - max value of state for all of its actions?\n",
    "        return new_q\n",
    "        \n",
    "    \n",
    "    def update_q_table(self, state, action, next_state, reward):\n",
    "        new_q = self.bellman_equation(state, action, next_state, reward)\n",
    "        self.q_table[state][action] = new_q\n",
    "        \n",
    "    \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7f3a6a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCNeuralNetwork(nn.Module):\n",
    "    \n",
    "    \n",
    "    def __init__(self, x_size, y_size, hidden_size, learning_rate):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.x_size = x_size\n",
    "        self.y_size = y_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        self.linear_relu_nn = nn.Sequential(\n",
    "            nn.Linear(x_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, y_size)\n",
    "        )\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(self.linear_relu_nn.parameters(), lr = self.learning_rate)\n",
    "        self.loss = torch.nn.MSELoss()\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.tensor(x, dtype=torch.float32)\n",
    "        out = self.linear_relu_nn(x)\n",
    "        return out\n",
    "        \n",
    "    \n",
    "    \n",
    "class ReplayBuffer:\n",
    "    \n",
    "    def __init__(self, size):\n",
    "        self.buffer = deque([], maxlen = size)\n",
    "        \n",
    "    def append(self, transition):\n",
    "        self.buffer.append(transition)\n",
    "        \n",
    "    def sample(self, sample_size):\n",
    "        return sample(self.buffer, sample_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "    \n",
    "    \n",
    " \n",
    "    \n",
    "\n",
    "class DQNAgent(): \n",
    "\n",
    "    \n",
    "    def __init__(self, num_actions, num_states, model, epsilon, use_replay_buffer=True, use_target_model=True):\n",
    "        \n",
    "        # Hyperparameters\n",
    "        self.sync_network_rate = 200\n",
    "        self.batch_size = 25\n",
    "        self.buffer_size = 10_000\n",
    "\n",
    "        self.num_states = num_states,\n",
    "        self.num_actions = num_actions\n",
    "        self.model = model\n",
    "        self.epsilon = epsilon\n",
    "        self.step_counter = 0\n",
    "\n",
    "\n",
    "        # Target model initialisation\n",
    "        if use_target_model == True:\n",
    "            self.target_model = type(model)(model.x_size, model.y_size,\n",
    "                                            model.hidden_size, model.learning_rate) # Creates another instance of that model\n",
    "            self.target_model.load_state_dict(self.model.state_dict()) # Transfers the weights\n",
    "            self.use_target_model = True\n",
    "        else:\n",
    "            self.use_target_model = False\n",
    "\n",
    "        # Replay buffer initialisation\n",
    "        if use_replay_buffer == True:\n",
    "            self.replay_buffer = ReplayBuffer(self.buffer_size)\n",
    "            self.use_replay_buffer = True\n",
    "        else:\n",
    "             self.use_replay_buffer = False\n",
    "\n",
    "    def choose_best_action(self, state):\n",
    "        if use_target_model == True:\n",
    "            action = argmax(self.target_model(state))\n",
    "        else:\n",
    "            action = argmax(self.model(state))\n",
    "        return action\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        if np.random.rand(1) < self.epsilon:\n",
    "            action = np.random.randint(0, self.num_actions)\n",
    "        else:\n",
    "            action = argmax(self.model(state))\n",
    "        return action\n",
    "\n",
    "    def sync_network(self):\n",
    "        if use_target_model == False:\n",
    "            return\n",
    "        elif sync_network_rate % step_counter == 0 and step_counter != 0:\n",
    "            self.target_model.load_state_dict(self.model.state_dict()) \n",
    "\n",
    "\n",
    "    def learn(self, samples):\n",
    "\n",
    "        self.sync_network()\n",
    "\n",
    "        if self.use_replay_buffer == True:\n",
    "            # this will batch samples from buffer together into the samples list\n",
    "            if len(self.replay_buffer) < batch_size:\n",
    "                return\n",
    "            samples = self.replay_buffer.sample(batch_size)\n",
    "\n",
    "\n",
    "        for sample in samples:\n",
    "\n",
    "            self.step_counter += 1\n",
    "\n",
    "            # Q value predicted for that state, action pair\n",
    "            predicted_value = self.model(state)[action] # predicted value = Q(s, a)\n",
    "\n",
    "            # Predict the next Q value using either the target network or the same network\n",
    "            if self.use_target_network == True:\n",
    "                predicted_next_value = self.target_model(next_state)\n",
    "            else:\n",
    "                predicted_next_value = self.model(next_state)\n",
    "\n",
    "            target_value = reward + self.discount_factor * max(predicted_next_value)  # target value = r + Î³Q(s', a')\n",
    "\n",
    "            # Loss function and backpropogation\n",
    "            loss = self.model.loss(predicted_value, target_value)\n",
    "            loss.backward()\n",
    "            self.model.optimizer.step()\n",
    "\n",
    "\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "## TO DO\n",
    "# FIGURE OUT LOSS FUNCTION AND BACKPROPOGATION - DONE\n",
    "# IMPLEMENT OPTIONAL BUFFER SAMPLING - DONE\n",
    "# IMPLEMENT OPTIONAL TARGET NETWORK - DONE\n",
    "# USE MATPLOTLIB OR SOMETHING TO VISUALIZE THE PERFORMANCE OF THE THREE AGENT TYPES\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1fb917",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0536e718",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "both arguments to matmul need to be at least 1D, but they are 0D and 2D",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[63], line 59\u001b[0m\n\u001b[1;32m     54\u001b[0m samples \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m terminated \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m truncated:\n\u001b[0;32m---> 59\u001b[0m     action \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mchoose_action(state)\n\u001b[1;32m     60\u001b[0m     new_state, reward, terminated, truncated, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m     62\u001b[0m     transition \u001b[38;5;241m=\u001b[39m (state, action, new_state, reward)\n",
      "Cell \u001b[0;32mIn[62], line 94\u001b[0m, in \u001b[0;36mDQNAgent.choose_action\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     92\u001b[0m     action \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_actions)\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 94\u001b[0m     action \u001b[38;5;241m=\u001b[39m argmax(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(state))\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m action\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[62], line 27\u001b[0m, in \u001b[0;36mFCNeuralNetwork.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     26\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(x, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m---> 27\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear_relu_nn(x)\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: both arguments to matmul need to be at least 1D, but they are 0D and 2D"
     ]
    }
   ],
   "source": [
    "episodes = 3000\n",
    "\n",
    "# Epsilon values\n",
    "start_epsilon = 0.8\n",
    "min_epsilon = 0.1\n",
    "epsilon_reduction = 0.1\n",
    "epsilon_decay = 100\n",
    "\n",
    "# Visualization\n",
    "render_decay = 1000\n",
    "\n",
    "# Environment setup\n",
    "env_name = \"FrozenLake-v1\"\n",
    "env = gym.make(env_name, render_mode=\"human\")\n",
    "num_actions = env.action_space.n\n",
    "num_states = env.observation_space.n\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "\n",
    "\n",
    "# Neural network hyperparameters\n",
    "hidden_size = 128\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Neural network setup\n",
    "model = FCNeuralNetwork(num_actions, num_states, hidden_size, learning_rate)\n",
    "\n",
    "\n",
    "# Agent setup\n",
    "agent_plain = DQNAgent(num_actions, num_states, model, start_epsilon, use_replay_buffer=False, use_target_model=False)\n",
    "agent_buffer = DQNAgent(num_actions, num_states, model, start_epsilon, use_replay_buffer=True, use_target_model=False)\n",
    "agent_buffer_target = DQNAgent(num_actions, num_states, model, start_epsilon, use_replay_buffer=True, use_target_model=True)\n",
    "\n",
    "agent_list = [agent_plain, agent_buffer, agent_buffer_target]\n",
    "\n",
    "\n",
    "for agent in agent_list:\n",
    "    \n",
    "\n",
    "    for episode in range(episodes):\n",
    "\n",
    "        if (episode + 1) % render_decay == 0:\n",
    "            env=gym.make(env_name, render_mode=\"human\")\n",
    "        else:\n",
    "            env=gym.make(env_name)\n",
    "\n",
    "        if (episode + 1) % epsilon_decay == 0 and (agent.epsilon - epsilon_reduction) > min_epsilon:\n",
    "            agent.epsilon -= epsilon_reduction\n",
    "\n",
    "\n",
    "        state, _ = env.reset()\n",
    "        terminated, truncated = False, False\n",
    "        samples = []\n",
    "\n",
    "        while not terminated and not truncated:\n",
    "            \n",
    "\n",
    "            action = agent.choose_action(state)\n",
    "            new_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            \n",
    "            transition = (state, action, new_state, reward)\n",
    "            \n",
    "            if agent.use_replay_buffer == True:\n",
    "                agent.replay_buffer.append(transition)\n",
    "            else: \n",
    "                samples.append(transition)\n",
    "                \n",
    "            agent.step_counter += 1\n",
    "                \n",
    "            if agent.step_counter % agent.batch_size == 0 and agent.step_counter != 0:\n",
    "                agent.learn(samples)\n",
    "                \n",
    "            state = new_state\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d5428be7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194d3713",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
